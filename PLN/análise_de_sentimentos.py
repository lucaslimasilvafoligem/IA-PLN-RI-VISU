# -*- coding: utf-8 -*-
"""Análise de sentimentos.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FE7JhHJNW4yBvQLeR09FqAQgEXWa9-CD
"""

!pip install unidecode

import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.neural_network import MLPClassifier

# Corpus Processing
import re
import nltk.corpus
from unidecode                        import unidecode
from nltk.tokenize                    import word_tokenize
from nltk                             import SnowballStemmer
from sklearn.feature_extraction.text  import TfidfVectorizer
from sklearn.preprocessing            import normalize

nltk.download('stopwords')
nltk.download('punkt')

"""# Lendo os dados

Os dados são compostos por 3000 comentários sobre diferentes marcas no Twitter.
"""

data = pd.read_csv('https://gist.githubusercontent.com/issilva5/993ff7c0d82fa3db85396740aaedbf63/raw/54242a8540a8c048775957f0f609ee5c774966bb/tweets.csv')

data[['sentiment', 'text']].head(10)

"""# Processamento dos dados"""

# removes a list of words (ie. stopwords) from a tokenized list.
def removeWords(listOfTokens, listOfWords):
    return [token for token in listOfTokens if token not in listOfWords]

# applies stemming to a list of tokenized words
def applyStemming(listOfTokens, stemmer):
    return [stemmer.stem(token) for token in listOfTokens]

# removes any words composed of less than 2 or more than 21 letters
def twoLetters(listOfTokens):
    twoLetterWord = []
    for token in listOfTokens:
        if len(token) <= 2 or len(token) >= 21:
            twoLetterWord.append(token)
    return twoLetterWord

def processCorpus(corpus, language):
    stopwords = nltk.corpus.stopwords.words(language)
    param_stemmer = SnowballStemmer(language)

    for document in corpus:
        index = corpus.index(document)
        corpus[index] = corpus[index].replace(u'\ufffd', '8')   # Replaces the ASCII '�' symbol with '8'
        corpus[index] = corpus[index].replace(',', '')          # Removes commas
        corpus[index] = corpus[index].rstrip('\n')              # Removes line breaks
        corpus[index] = corpus[index].casefold()                # Makes all letters lowercase

        corpus[index] = re.sub('\W_',' ', corpus[index])        # removes specials characters and leaves only words
        corpus[index] = re.sub("\S*\d\S*"," ", corpus[index])   # removes numbers and words concatenated with numbers IE h4ck3r. Removes road names such as BR-381.
        corpus[index] = re.sub("\S*@\S*\s?"," ", corpus[index]) # removes emails and mentions (words with @)
        corpus[index] = re.sub(r'http\S+', '', corpus[index])   # removes URLs with http
        corpus[index] = re.sub(r'www\S+', '', corpus[index])    # removes URLs with www

        listOfTokens = word_tokenize(corpus[index])
        twoLetterWord = twoLetters(listOfTokens)

        listOfTokens = removeWords(listOfTokens, stopwords)
        listOfTokens = removeWords(listOfTokens, twoLetterWord)

        listOfTokens = applyStemming(listOfTokens, param_stemmer)

        corpus[index]   = " ".join(listOfTokens)
        corpus[index] = unidecode(corpus[index])

    return corpus

teste = "oioio!!, poOpo"
re.sub('\W_',' ',teste)
#word_tokenize(teste.casefold())

corpus = processCorpus(data['text'].tolist(), 'english')

print('Texto original:', data['text'].tolist()[563])
print()
print('Texto processado:', corpus[563])
print()
print('-'*12)
print()
print('Texto original:', data['text'].tolist()[1876])
print()
print('Texto processado:', corpus[1876])

"""# Treinando o modelo de Regressão Logística

Vetorização do texto processado:
"""

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(corpus)
y = data['sentiment'].to_numpy()

"""Separação em treino e teste, na proporção 70/30:"""

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.7)

"""Treinando o modelo (.fit):"""

clr = LogisticRegression(random_state=0).fit(X_train, y_train)
cnb = MultinomialNB().fit(X_train, y_train)
cmlp = MLPClassifier(random_state=0, max_iter=300, hidden_layer_sizes=(128,16,)).fit(X_train, y_train)

"""# Prevendo e avaliando

Para prever o identificador da classe:
"""

clr.predict(X_test)[:10]

cnb.predict(X_test)[:10]

cmlp.predict(X_test)[:10]

"""Para prever probabilidade de pertencer a cada classe:"""

clr.classes_

clr.predict_proba(X_test)[:10]

cnb.predict_proba(X_test)[:10]

cmlp.predict_proba(X_test)[:10]

"""## Matriz de confusão

A matriz de confusão fornece um meio de avaliar o êxito de um problema de classificação e onde ele comete erros (ou seja, onde ele se torna "confuso").
"""

import seaborn as sns

def plot_confusion_matrix(y_true, y_pred, labels=None) -> None:
    cm = confusion_matrix(y_true, y_pred, labels=labels)

    plt.figure(figsize=(len(labels), len(labels)))
    sns.heatmap(
        cm,
        annot=True,
        fmt='d',
        cmap='Blues',
        xticklabels=labels,
        yticklabels=labels,
    )
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title('Confusion Matrix')
    plt.show()

plot_confusion_matrix(y_test, clr.predict(X_test), labels=['Negative', 'Positive'])

plot_confusion_matrix(y_test, cnb.predict(X_test), labels=['Negative', 'Positive'])

plot_confusion_matrix(y_test, cmlp.predict(X_test), labels=['Negative', 'Positive'])

"""## Relatório de classificação"""

print(classification_report(y_test, clr.predict(X_test)))

print(classification_report(y_test, cnb.predict(X_test)))

print(classification_report(y_test, cmlp.predict(X_test)))