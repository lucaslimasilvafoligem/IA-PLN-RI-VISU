# -*- coding: utf-8 -*-
"""NER com sklearn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i0Y3dytjSp-SGSzBrCoXRfevvhWnyA0V
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction import DictVectorizer
from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report

df = pd.read_csv('https://github.com/susanli2016/NLP-with-Python/raw/master/data/ner_dataset.csv', encoding = "ISO-8859-1")
df = df[:50000]
df.head()

df = df.fillna(method='ffill')

"""A seguir podemos ver a quantidade de setenças, palavras e tags únicas."""

df['Sentence #'].nunique(), df.Word.nunique(), df.Tag.nunique()

"""Assim como a distribuição das tags:"""

df.groupby('Tag').size().reset_index(name='counts')

X = df.drop('Tag', axis=1)
X.head()

v = DictVectorizer(sparse=False)
X = v.fit_transform(X.to_dict('records'))
X.shape

y = df.Tag.values

classes = np.unique(y)

classes = classes.tolist()
classes

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, random_state=0)

"""### Multi-Layer Perceptron"""

new_classes = classes.copy()
new_classes.pop()

per = MLPClassifier(verbose=10, max_iter=10)
per.fit(X_train, y_train)

print(classification_report(y_pred=per.predict(X_test), y_true=y_test, labels=new_classes))

"""### Regressão Logística"""

lr = LogisticRegression()
lr.fit(X_train, y_train)

print(classification_report(y_pred=lr.predict(X_test), y_true=y_test, labels=new_classes))

"""### Naive Bayes"""

nb = MultinomialNB()
nb.fit(X_train, y_train)

print(classification_report(y_pred=nb.predict(X_test), y_true=y_test, labels = new_classes))